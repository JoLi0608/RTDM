{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:148v142b) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2504... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03648cf4d4a14818974f5df6fb955747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">colorful-bush-8</strong>: <a href=\"https://wandb.ai/lwyjo/PPO/runs/148v142b\" target=\"_blank\">https://wandb.ai/lwyjo/PPO/runs/148v142b</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220203_171043-148v142b/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:148v142b). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/lwyjo/PPO/runs/17wwguk8\" target=\"_blank\">lilac-dream-9</a></strong> to <a href=\"https://wandb.ai/lwyjo/PPO\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/lwyjo/PPO/runs/17wwguk8?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa0efd6b8e0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"PPO\", entity=\"lwyjo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import pickle\n",
    "import torch\n",
    "from ppo import PPOAgent\n",
    "from utils import collect_trajectories, random_sample\n",
    "from parallelEnv import parallelEnv\n",
    "\n",
    "gamma = .99\n",
    "gae_lambda = 0.95\n",
    "use_gae = True\n",
    "\n",
    "nenvs = 8\n",
    "rollout_length = 200\n",
    "minibatches = 10*8\n",
    "# Calculate the batch_size\n",
    "nbatch = nenvs * rollout_length\n",
    "optimization_epochs = 4\n",
    "seed=1\n",
    "\n",
    "config = wandb.config\n",
    "config.gamma = gamma\n",
    "config.nenvs = nenvs\n",
    "config.seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-42:\n",
      "Process Process-41:\n",
      "Process Process-39:\n",
      "Process Process-40:\n",
      "Process Process-38:\n",
      "Process Process-36:\n",
      "Process Process-37:\n",
      "Process Process-35:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/liwenyu/Documents/GitHub/RTDM/parallelEnv.py\", line 98, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 388, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/liwenyu/Documents/GitHub/RTDM/parallelEnv.py\", line 98, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 388, in _recv\n",
      "    raise EOFError\n",
      "  File \"/Users/liwenyu/Documents/GitHub/RTDM/parallelEnv.py\", line 98, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "EOFError\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 388, in _recv\n",
      "    raise EOFError\n",
      "Traceback (most recent call last):\n",
      "EOFError\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/liwenyu/Documents/GitHub/RTDM/parallelEnv.py\", line 98, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 388, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/liwenyu/Documents/GitHub/RTDM/parallelEnv.py\", line 98, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 388, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/liwenyu/Documents/GitHub/RTDM/parallelEnv.py\", line 98, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/liwenyu/Documents/GitHub/RTDM/parallelEnv.py\", line 98, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 388, in _recv\n",
      "    raise EOFError\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 388, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/liwenyu/Documents/GitHub/RTDM/parallelEnv.py\", line 98, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 388, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train(episode,env_name):\n",
    "    # gamma = .99\n",
    "    # gae_lambda = 0.95\n",
    "    # use_gae = True\n",
    "    beta = .01\n",
    "    cliprange = 0.1\n",
    "    best_score = -np.inf\n",
    "    goal_score = 195.0\n",
    "    #\n",
    "    # nenvs = 8\n",
    "    # rollout_length = 200\n",
    "    # minibatches = 10*8\n",
    "    # # Calculate the batch_size\n",
    "    # nbatch = nenvs * rollout_length\n",
    "    # optimization_epochs = 4\n",
    "\n",
    "\n",
    "\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    envs = parallelEnv(env_name, nenvs, seed=seed)\n",
    "    agent = PPOAgent(state_size=envs.observation_space.shape[0],\n",
    "                     action_size=envs.action_space.n, \n",
    "                     seed=0,\n",
    "                     hidden_layers=[64,64],\n",
    "                     lr_policy=1e-4, \n",
    "                     use_reset=True,\n",
    "                     device=device)\n",
    "    print(\"------------------\")\n",
    "    print(agent.policy)\n",
    "    print(\"------------------\")\n",
    "\n",
    "    # keep track of progress\n",
    "    mean_rewards = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    loss_storage = []\n",
    "\n",
    "    for i_episode in range(episode+1):\n",
    "        log_probs_old, states, actions, rewards, values, dones, vals_last = collect_trajectories(envs, agent.policy, rollout_length)\n",
    "\n",
    "        returns = np.zeros_like(rewards)\n",
    "        advantages = np.zeros_like(rewards)\n",
    "        \n",
    "        if not use_gae:\n",
    "            for t in reversed(range(rollout_length)):\n",
    "                if t == rollout_length - 1:\n",
    "                    returns[t] = rewards[t] + gamma * (1-dones[t]) * vals_last\n",
    "                else:\n",
    "                    returns[t] = rewards[t] + gamma * (1-dones[t]) * returns[t+1]\n",
    "                advantages[t] = returns[t] - values[t]  #calculation of advantege funvtion LECTURE 7\n",
    "        else:\n",
    "            for t in reversed(range(rollout_length)):\n",
    "                if t == rollout_length - 1:\n",
    "                    returns[t] = rewards[t] + gamma * (1-dones[t]) * vals_last\n",
    "                    td_error = returns[t] - values[t] # td error 与GAE关系\n",
    "                else:\n",
    "                    returns[t] = rewards[t] + gamma * (1-dones[t]) * returns[t+1]\n",
    "                    td_error = rewards[t] + gamma * (1-dones[t]) * values[t+1] - values[t]\n",
    "                advantages[t] = advantages[t] * gae_lambda * gamma * (1-dones[t]) + td_error\n",
    "        \n",
    "        # convert to pytorch tensors and move to gpu if available\n",
    "        returns = torch.from_numpy(returns).float().to(device).view(-1,)\n",
    "        advantages = torch.from_numpy(advantages).float().to(device).view(-1,)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "\n",
    "        wandb.log({\"returns\": returns})\n",
    "        wandb.log({\"advantages\": advantages})\n",
    "        \n",
    "        for _ in range(optimization_epochs):\n",
    "            sampler = random_sample(nbatch, minibatches)\n",
    "            for inds in sampler:\n",
    "                mb_log_probs_old = log_probs_old[inds]\n",
    "                mb_states = states[inds]\n",
    "                mb_actions = actions[inds]\n",
    "                mb_returns = returns[inds]\n",
    "                mb_advantages = advantages[inds]\n",
    "                loss_p, loss_v, loss_ent = agent.update(mb_log_probs_old, mb_states, mb_actions, mb_returns, mb_advantages, cliprange=cliprange, beta=beta)\n",
    "                # wandb.log({\"loss_p\": loss_p})\n",
    "                # wandb.log({\"loss_v\": loss_v})\n",
    "                # wandb.log({\"loss_ent\": loss_ent})\n",
    "                loss_storage.append([loss_p, loss_v, loss_ent])\n",
    "                \n",
    "        total_rewards = np.sum(rewards, axis=0)\n",
    "        scores_window.append(np.mean(total_rewards)) # last 100 scores\n",
    "        # mean_reward = torch.from_numpy(np.mean(total_rewards)).float().to(device).view(-1,)\n",
    "        # mean_reward = int(np.mean(total_rewards))\n",
    "        mean_rewards.append(np.mean(total_rewards))  # get the average reward of the parallel environments\n",
    "        wandb.log({\"total_rewards\": mean_rewards})\n",
    "        cliprange*=.999                              # the clipping parameter reduces as time goes on\n",
    "        beta*=.999                                   # the regulation term reduces\n",
    "    \n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            print(total_rewards)\n",
    "        if np.mean(scores_window)>=goal_score and np.mean(scores_window)>=best_score:            \n",
    "            torch.save(agent.policy.state_dict(), \"policy_cartpole.pth\")\n",
    "            best_score = np.mean(scores_window)\n",
    "    \n",
    "    return mean_rewards, loss_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'feature_dim' and 'input_channel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yt/3gz8yy8s1ks3w9hgqhvppyyc0000gn/T/ipykernel_2387/1866149295.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'CartPole-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(mean_rewards,loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/yt/3gz8yy8s1ks3w9hgqhvppyyc0000gn/T/ipykernel_2387/3074857515.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(episode, env_name)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallelEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnenvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     agent = PPOAgent(state_size=envs.observation_space.shape[0],\n\u001b[0m\u001b[1;32m     23\u001b[0m                      \u001b[0maction_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                      \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'feature_dim' and 'input_channel'"
     ]
    }
   ],
   "source": [
    "\n",
    "mean_rewards, loss = train(400,'CartPole-v0')\n",
    "#print(mean_rewards,loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_rewards)\n",
    "plt.ylabel('Average score')\n",
    "plt.xlabel('episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppo import PPOAgent\n",
    "import torch\n",
    "import gym\n",
    "\n",
    "env= gym.make('CartPole-v0')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = PPOAgent(state_size=env.observation_space.shape[0],\n",
    "                 action_size=env.action_space.n, \n",
    "                 seed=0,\n",
    "                 hidden_layers=[64,64],\n",
    "                 lr_policy=1e-4, \n",
    "                 use_reset=True,\n",
    "                 device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy.load_state_dict(torch.load('policy_cartpole.pth', map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation, rc\n",
    "rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rand\n",
    "\n",
    "# function to animate a list of frames\n",
    "def animate_frames(frames):\n",
    "    plt.axis('off')\n",
    "\n",
    "    # color option for plotting\n",
    "    # use Greys for greyscale\n",
    "    cmap = None if len(frames[0].shape)==3 else 'Greys'\n",
    "    patch = plt.imshow(frames[0], cmap=cmap)  \n",
    "\n",
    "    fanim = animation.FuncAnimation(plt.gcf(), \\\n",
    "        lambda x: patch.set_data(frames[x]), frames = len(frames), interval=50)\n",
    "    \n",
    "    return fanim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def play(env, policy, times, asy = 0, level = 0):\n",
    "    print(level)\n",
    "    rewards = []\n",
    "\n",
    "    for k in range(10):\n",
    "        frame1 = env.reset()\n",
    "        reward = 0\n",
    "\n",
    "        anim_frames = []\n",
    "\n",
    "        for i in range(times):\n",
    "\n",
    "            anim_frames.append(env.render(mode='rgb_array'))\n",
    "            frame_input = torch.from_numpy(frame1).unsqueeze(0).float().to(device)\n",
    "            t1 = time.time()\n",
    "            action = policy.act(frame_input)['a'].cpu().numpy()\n",
    "            t2 = time.time()\n",
    "            frame1, _, is_done, _ = env.step(int(action))\n",
    "            repeat = level * 1000*(t2 - t1)\n",
    "            #print(repeat)\n",
    "            reward += 1\n",
    "            if asy:\n",
    "                \n",
    "                for j in range(int(repeat)):\n",
    "                    frame1, _, is_done, _ = env.step(int(action))\n",
    "                    reward += 1\n",
    "                    if is_done:\n",
    "                        #print(\"reward :\", reward)\n",
    "                        rewards.append(reward)\n",
    "                        #print(rewards)\n",
    "                        \n",
    "            else:\n",
    "                if is_done:\n",
    "                    #print(\"reward :\", reward)\n",
    "                    rewards.append(reward)\n",
    "                    #print(rewards)\n",
    "                    \n",
    "            if is_done:\n",
    "                break\n",
    "    \n",
    "        env.close()\n",
    "    reward_ave = sum(rewards)/len(rewards)\n",
    "    return animate_frames(anim_frames), reward_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = []\n",
    "_, reward = play(env, agent.policy, 250, asy = 0)\n",
    "record.append(reward)\n",
    "x = range(0,15)\n",
    "for level in x[1:]:\n",
    "    _, reward = play(env, agent.policy, 250, asy = 1, level = level)\n",
    "    record.append(reward)\n",
    "\n",
    "print(record)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
  },
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
