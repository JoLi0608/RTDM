Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 13       |
|    ep_rew_mean        | 13       |
| time/                 |          |
|    fps                | 1105     |
|    iterations         | 100      |
|    time_elapsed       | 0        |
|    total_timesteps    | 500      |
| train/                |          |
|    entropy_loss       | -0.348   |
|    explained_variance | 0.702    |
|    learning_rate      | 0.0007   |
|    n_updates          | 99       |
|    policy_loss        | 0.105    |
|    value_loss         | 1.26     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 12.6     |
|    ep_rew_mean        | 12.6     |
| time/                 |          |
|    fps                | 1074     |
|    iterations         | 200      |
|    time_elapsed       | 0        |
|    total_timesteps    | 1000     |
| train/                |          |
|    entropy_loss       | -0.498   |
|    explained_variance | 0.626    |
|    learning_rate      | 0.0007   |
|    n_updates          | 199      |
|    policy_loss        | 0.0191   |
|    value_loss         | 11.1     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 14.2     |
|    ep_rew_mean        | 14.2     |
| time/                 |          |
|    fps                | 1064     |
|    iterations         | 300      |
|    time_elapsed       | 1        |
|    total_timesteps    | 1500     |
| train/                |          |
|    entropy_loss       | -0.683   |
|    explained_variance | -0.00441 |
|    learning_rate      | 0.0007   |
|    n_updates          | 299      |
|    policy_loss        | 1.71     |
|    value_loss         | 8.1      |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 16.5     |
|    ep_rew_mean        | 16.5     |
| time/                 |          |
|    fps                | 1077     |
|    iterations         | 400      |
|    time_elapsed       | 1        |
|    total_timesteps    | 2000     |
| train/                |          |
|    entropy_loss       | -0.69    |
|    explained_variance | 0.0513   |
|    learning_rate      | 0.0007   |
|    n_updates          | 399      |
|    policy_loss        | 1.57     |
|    value_loss         | 7.16     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 19.4     |
|    ep_rew_mean        | 19.4     |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 500      |
|    time_elapsed       | 2        |
|    total_timesteps    | 2500     |
| train/                |          |
|    entropy_loss       | -0.656   |
|    explained_variance | 0.287    |
|    learning_rate      | 0.0007   |
|    n_updates          | 499      |
|    policy_loss        | 1.46     |
|    value_loss         | 5.2      |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 22.1     |
|    ep_rew_mean        | 22.1     |
| time/                 |          |
|    fps                | 1056     |
|    iterations         | 600      |
|    time_elapsed       | 2        |
|    total_timesteps    | 3000     |
| train/                |          |
|    entropy_loss       | -0.677   |
|    explained_variance | 0.0109   |
|    learning_rate      | 0.0007   |
|    n_updates          | 599      |
|    policy_loss        | 1.12     |
|    value_loss         | 5.39     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 25.6     |
|    ep_rew_mean        | 25.6     |
| time/                 |          |
|    fps                | 1053     |
|    iterations         | 700      |
|    time_elapsed       | 3        |
|    total_timesteps    | 3500     |
| train/                |          |
|    entropy_loss       | -0.681   |
|    explained_variance | 0.0024   |
|    learning_rate      | 0.0007   |
|    n_updates          | 699      |
|    policy_loss        | 1.24     |
|    value_loss         | 4.89     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 29.6     |
|    ep_rew_mean        | 29.6     |
| time/                 |          |
|    fps                | 1059     |
|    iterations         | 800      |
|    time_elapsed       | 3        |
|    total_timesteps    | 4000     |
| train/                |          |
|    entropy_loss       | -0.651   |
|    explained_variance | 0.00665  |
|    learning_rate      | 0.0007   |
|    n_updates          | 799      |
|    policy_loss        | 1.15     |
|    value_loss         | 4.5      |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 32.8     |
|    ep_rew_mean        | 32.8     |
| time/                 |          |
|    fps                | 1055     |
|    iterations         | 900      |
|    time_elapsed       | 4        |
|    total_timesteps    | 4500     |
| train/                |          |
|    entropy_loss       | -0.598   |
|    explained_variance | 0.0011   |
|    learning_rate      | 0.0007   |
|    n_updates          | 899      |
|    policy_loss        | 0.891    |
|    value_loss         | 3.98     |
------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 36.7      |
|    ep_rew_mean        | 36.7      |
| time/                 |           |
|    fps                | 1063      |
|    iterations         | 1000      |
|    time_elapsed       | 4         |
|    total_timesteps    | 5000      |
| train/                |           |
|    entropy_loss       | -0.559    |
|    explained_variance | -0.000655 |
|    learning_rate      | 0.0007    |
|    n_updates          | 999       |
|    policy_loss        | 1.03      |
|    value_loss         | 3.49      |
-------------------------------------
difficulty level: 0
[83.0, 235.0, 181.0, 146.0, 147.0, 75.0, 69.0, 63.0, 115.0, 55.0, 86.0, 93.0, 141.0, 139.0, 206.0, 145.0, 76.0, 84.0, 161.0, 155.0]
difficulty level: 1
/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/gym/logger.py:34: UserWarning: [33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.
  warnings.warn(colorize("%s: %s" % ("WARN", msg % args), "yellow"))
[75.0, 85.0, 119.0, 99.0, 123.0, 84.0, 139.0, 91.0, 151.0, 73.0, 222.0, 102.0, 100.0, 126.0, 87.0, 113.0, 80.0, 71.0, 264.0, 87.0]
difficulty level: 2
[64.0, 66.0, 83.0, 71.0, 111.0, 86.0, 72.0, 69.0, 105.0, 70.0, 96.0, 113.0, 181.0, 35.0, 55.0, 81.0, 72.0, 72.0, 69.0, 62.0]
difficulty level: 3
[49.0, 32.0, 40.0, 33.0, 55.0, 13.0, 46.0, 44.0, 55.0, 90.0, 94.0, 31.0, 45.0, 54.0, 71.0, 46.0, 46.0, 50.0, 27.0, 83.0]
difficulty level: 4
[28.0, 96.0, 57.0, 47.0, 129.0, 76.0, 55.0, 140.0, 80.0, 77.0, 73.0, 96.0, 47.0, 67.0, 56.0, 92.0, 107.0, 104.0, 64.0, 124.0]
difficulty level: 5
[17.0, 83.0, 11.0, 59.0, 53.0, 46.0, 90.0, 53.0, 38.0, 51.0, 23.0, 45.0, 105.0, 91.0, 96.0, 28.0, 44.0, 61.0, 65.0, 21.0]
difficulty level: 6
[44.0, 71.0, 36.0, 49.0, 16.0, 21.0, 55.0, 54.0, 58.0, 22.0, 58.0, 138.0, 51.0, 74.0, 63.0, 31.0, 66.0, 39.0, 52.0, 48.0]
difficulty level: 7
[23.0, 60.0, 28.0, 27.0, 17.0, 53.0, 9.0, 34.0, 43.0, 37.0, 81.0, 30.0, 85.0, 41.0, 35.0, 33.0, 34.0, 41.0, 51.0, 58.0]
difficulty level: 8
[56.0, 24.0, 22.0, 9.0, 19.0, 54.0, 54.0, 35.0, 42.0, 23.0, 38.0, 45.0, 34.0, 32.0, 61.0, 60.0, 20.0, 70.0, 40.0, 90.0]
difficulty level: 9
[17.0, 28.0, 23.0, 10.0, 34.0, 8.0, 10.0, 17.0, 12.0, 18.0, 54.0, 37.0, 45.0, 20.0, 9.0, 61.0, 34.0, 17.0, 68.0, 16.0]
difficulty level: 10
[23.0, 61.0, 20.0, 22.0, 10.0, 10.0, 30.0, 29.0, 57.0, 40.0, 25.0, 36.0, 10.0, 43.0, 11.0, 40.0, 9.0, 87.0, 34.0, 10.0]
difficulty level: 11
[10.0, 79.0, 28.0, 10.0, 33.0, 11.0, 71.0, 63.0, 29.0, 42.0, 50.0, 11.0, 22.0, 28.0, 53.0, 10.0, 11.0, 11.0, 95.0, 29.0]
difficulty level: 12
[12.0, 10.0, 22.0, 11.0, 13.0, 11.0, 12.0, 50.0, 9.0, 44.0, 10.0, 18.0, 26.0, 10.0, 39.0, 9.0, 10.0, 10.0, 9.0, 27.0]
difficulty level: 13
[9.0, 9.0, 10.0, 12.0, 10.0, 10.0, 11.0, 10.0, 9.0, 55.0, 9.0, 10.0, 10.0, 10.0, 10.0, 9.0, 23.0, 38.0, 46.0, 9.0]
difficulty level: 14
[10.0, 9.0, 16.0, 8.0, 13.0, 11.0, 13.0, 10.0, 9.0, 11.0, 11.0, 38.0, 11.0, 10.0, 10.0, 10.0, 9.0, 9.0, 9.0, 10.0]
difficulty level: 15
[10.0, 10.0, 9.0, 10.0, 10.0, 10.0, 10.0, 9.0, 10.0, 10.0, 10.0, 9.0, 9.0, 13.0, 20.0, 10.0, 10.0, 11.0, 13.0, 11.0]
difficulty level: 16
[9.0, 11.0, 9.0, 10.0, 9.0, 10.0, 9.0, 10.0, 10.0, 10.0, 9.0, 10.0, 10.0, 10.0, 9.0, 10.0, 17.0, 9.0, 9.0, 9.0]
difficulty level: 17
[10.0, 10.0, 10.0, 10.0, 12.0, 10.0, 8.0, 10.0, 8.0, 9.0, 11.0, 11.0, 9.0, 10.0, 9.0, 9.0, 9.0, 11.0, 10.0, 10.0]
difficulty level: 18
[9.0, 9.0, 10.0, 10.0, 9.0, 10.0, 10.0, 9.0, 10.0, 9.0, 10.0, 11.0, 8.0, 10.0, 11.0, 9.0, 10.0, 10.0, 8.0, 11.0]
difficulty level: 19
[11.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.0, 10.0, 9.0, 10.0, 8.0, 10.0, 9.0, 9.0, 10.0, 10.0, 11.0, 8.0, 9.0, 11.0]
final result: [122.75, 114.55, 81.65, 50.2, 80.75, 54.0, 52.3, 41.0, 41.4, 26.9, 30.35, 34.8, 18.1, 15.95, 11.85, 10.7, 9.95, 9.8, 9.65, 9.7]