
2022-04-01 12:42:48,899	INFO services.py:1412 -- View the Ray dashboard at [32m[1mhttp://127.0.0.1:8265
== Status ==
Current time: 2022-04-01 12:42:52 (running for 00:00:00.19)
Memory usage on this node: 9.7/16.0 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/5.04 GiB heap, 0.0/2.0 GiB objects
Result logdir: /Users/liwenyu/Downloads/ray_results /PPO
Number of trials: 1/1 (1 PENDING)
+---------------------------+----------+-------+
| Trial name                | status   | loc   |
|---------------------------+----------+-------|
| PPO_Hopper-v2_dce5e_00000 | PENDING  |       |
+---------------------------+----------+-------+
[36m(PPOTrainer pid=26468)[39m 2022-04-01 12:42:56,961	INFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.
[36m(PPOTrainer pid=26468)[39m 2022-04-01 12:42:56,961	INFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
== Status ==
Current time: 2022-04-01 12:43:02 (running for 00:00:10.10)
Memory usage on this node: 9.9/16.0 GiB
Using FIFO scheduling algorithm.
Resources requested: 3.0/8 CPUs, 0/0 GPUs, 0.0/5.04 GiB heap, 0.0/2.0 GiB objects
Result logdir: /Users/liwenyu/Downloads/ray_results /PPO
Number of trials: 1/1 (1 RUNNING)
+---------------------------+----------+-----------------+
| Trial name                | status   | loc             |
|---------------------------+----------+-----------------|
| PPO_Hopper-v2_dce5e_00000 | RUNNING  | 127.0.0.1:26468 |
+---------------------------+----------+-----------------+
== Status ==
Current time: 2022-04-01 12:43:03 (running for 00:00:11.12)
Memory usage on this node: 9.9/16.0 GiB
Using FIFO scheduling algorithm.
Resources requested: 3.0/8 CPUs, 0/0 GPUs, 0.0/5.04 GiB heap, 0.0/2.0 GiB objects
Result logdir: /Users/liwenyu/Downloads/ray_results /PPO
Number of trials: 1/1 (1 RUNNING)
+---------------------------+----------+-----------------+
| Trial name                | status   | loc             |
|---------------------------+----------+-----------------|
| PPO_Hopper-v2_dce5e_00000 | RUNNING  | 127.0.0.1:26468 |
+---------------------------+----------+-----------------+
[36m(PPOTrainer pid=26468)[39m 2022-04-01 12:43:02,118	WARNING util.py:55 -- Install gputil for GPU system monitoring.
[36m(PPOTrainer pid=26468)[39m 2022-04-01 12:43:02,140	INFO trainable.py:495 -- Restored on 127.0.0.1 from checkpoint: /Users/liwenyu/Downloads/ray_results /PPO/PPO_Hopper-v2_dce5e_00000_0_2022-04-01_12-42-52/tmphdd8ol8prestore_from_object/checkpoint-4300
[36m(PPOTrainer pid=26468)[39m 2022-04-01 12:43:02,141	INFO trainable.py:503 -- Current state after restoring: {'_iteration': 4300, '_timesteps_total': 17200000, '_time_total': 66449.50474596024, '_episodes_total': 78698}
== Status ==
Current time: 2022-04-01 12:43:08 (running for 00:00:16.13)
Memory usage on this node: 9.9/16.0 GiB
Using FIFO scheduling algorithm.
Resources requested: 3.0/8 CPUs, 0/0 GPUs, 0.0/5.04 GiB heap, 0.0/2.0 GiB objects
Result logdir: /Users/liwenyu/Downloads/ray_results /PPO
Number of trials: 1/1 (1 RUNNING)
+---------------------------+----------+-----------------+
| Trial name                | status   | loc             |
|---------------------------+----------+-----------------|
| PPO_Hopper-v2_dce5e_00000 | RUNNING  | 127.0.0.1:26468 |
+---------------------------+----------+-----------------+
Result for PPO_Hopper-v2_dce5e_00000:
  agent_timesteps_total: 17204000
  custom_metrics: {}
  date: 2022-04-01_12-43-11
  done: true
  episode_len_mean: 174.54545454545453
  episode_media: {}
  episode_reward_max: 841.3496395938948
  episode_reward_mean: 568.400261614922
  episode_reward_min: 70.5632356474586
  episodes_this_iter: 22
  episodes_total: 78720
  experiment_id: 1f82ae63d4e242daa3e6ce4fe971dddc
  hostname: dhcp-10-249-173-232.eduroam.wireless.private.cam.ac.uk
  info:
    learner:
      default_policy:
        custom_metrics: {}
        learner_stats:
          allreduce_latency: 0.0
          cur_kl_coeff: 0.20000000000000004
          cur_lr: 5.0000000000000016e-05
          entropy: 10.431650335814362
          entropy_coeff: 0.0
          kl: 0.04074273849362317
          policy_loss: -0.08715975158397228
          total_loss: 1940.8761736880067
          vf_explained_var: 0.5829570636313449
          vf_loss: 1940.9551843581662
        model: {}
    num_agent_steps_sampled: 17204000
    num_agent_steps_trained: 17204000
    num_steps_sampled: 17204000
    num_steps_trained: 17204000
    num_steps_trained_this_iter: 4000
  iterations_since_restore: 1
  node_ip: 127.0.0.1
  num_healthy_workers: 2
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.27692307692308
    ram_util_percent: 61.96923076923076
  pid: 26468
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1203364458517811
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.23506117844093086
    mean_inference_ms: 0.8864263365829902
    mean_raw_obs_processing_ms: 0.10588304928575143
  time_since_restore: 9.023401021957397
  time_this_iter_s: 9.023401021957397
  time_total_s: 66458.5281469822
  timers:
    learn_throughput: 651.821
    learn_time_ms: 6136.651
    load_throughput: 10492317.699
    load_time_ms: 0.381
    sample_throughput: 1375.785
    sample_time_ms: 2907.43
    update_time_ms: 1.746
  timestamp: 1648813391
  timesteps_since_restore: 4000
  timesteps_this_iter: 4000
  timesteps_total: 17204000
  training_iteration: 4301
  trial_id: dce5e_00000
== Status ==
Current time: 2022-04-01 12:43:11 (running for 00:00:19.20)
Memory usage on this node: 9.9/16.0 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/5.04 GiB heap, 0.0/2.0 GiB objects
Result logdir: /Users/liwenyu/Downloads/ray_results /PPO
Number of trials: 1/1 (1 TERMINATED)
+---------------------------+------------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name                | status     | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|---------------------------+------------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_Hopper-v2_dce5e_00000 | TERMINATED | 127.0.0.1:26468 |   4301 |          66458.5 | 17204000 |    568.4 |               841.35 |              70.5632 |            174.545 |
+---------------------------+------------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
[36m(PPOTrainer pid=26468)[39m 2022-04-01 12:43:11,198	WARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!
2022-04-01 12:43:11,357	INFO tune.py:639 -- Total run time: 19.35 seconds (19.18 seconds for the tuning loop).
Traceback (most recent call last):
  File "/Users/liwenyu/Documents/GitHub/RTDM/loadmodel.py", line 123, in <module>
    reward_ave = play(env, trainer, 800, asy = 0)
NameError: name 'trainer' is not defined
Traceback (most recent call last):
  File "/Users/liwenyu/Documents/GitHub/RTDM/loadmodel.py", line 123, in <module>
    reward_ave = play(env, trainer, 800, asy = 0)
NameError: name 'trainer' is not defined