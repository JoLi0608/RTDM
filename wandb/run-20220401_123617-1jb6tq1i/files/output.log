
2022-04-01 12:36:24,910	INFO services.py:1412 -- View the Ray dashboard at [32m[1mhttp://127.0.0.1:8265
== Status ==
Current time: 2022-04-01 12:36:28 (running for 00:00:00.18)
Memory usage on this node: 9.3/16.0 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/5.53 GiB heap, 0.0/2.0 GiB objects
Result logdir: /Users/liwenyu/Downloads/ray_results /PPO
Number of trials: 1/1 (1 PENDING)
+-----------------+----------+-------+
| Trial name      | status   | loc   |
|-----------------+----------+-------|
| PPO_f81e5_00000 | PENDING  |       |
+-----------------+----------+-------+
[36m(PPOTrainer pid=26123)[39m 2022-04-01 12:36:33,135	INFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.
[36m(PPOTrainer pid=26123)[39m 2022-04-01 12:36:33,136	INFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.
[36m(PPOTrainer pid=26123)[39m 2022-04-01 12:36:33,136	INFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
== Status ==
Current time: 2022-04-01 12:36:37 (running for 00:00:09.64)
Memory usage on this node: 10.0/16.0 GiB
Using FIFO scheduling algorithm.
Resources requested: 3.0/8 CPUs, 0/0 GPUs, 0.0/5.53 GiB heap, 0.0/2.0 GiB objects
Result logdir: /Users/liwenyu/Downloads/ray_results /PPO
Number of trials: 1/1 (1 RUNNING)
+-----------------+----------+-------+
| Trial name      | status   | loc   |
|-----------------+----------+-------|
| PPO_f81e5_00000 | RUNNING  |       |
+-----------------+----------+-------+
2022-04-01 12:36:37,839	ERROR trial_runner.py:1092 -- Trial PPO_f81e5_00000: Error processing restore.
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/tune/trial_runner.py", line 1085, in _process_trial_restore
    self.trial_executor.fetch_result(trial)
  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/tune/ray_trial_executor.py", line 675, in fetch_result
    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)
  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/worker.py", line 1765, in get
    raise value
ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, [36mray::PPOTrainer.__init__()[39m (pid=26123, ip=127.0.0.1, repr=PPOTrainer)
  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py", line 925, in _init
    raise NotImplementedError
NotImplementedError
During handling of the above exception, another exception occurred:
[36mray::PPOTrainer.__init__()[39m (pid=26123, ip=127.0.0.1, repr=PPOTrainer)
  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py", line 746, in __init__
    super().__init__(config, logger_creator, remote_checkpoint_dir,
  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/tune/trainable.py", line 124, in __init__
    self.setup(copy.deepcopy(self.config))
  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py", line 822, in setup
    self.workers = self._make_workers(
  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py", line 1995, in _make_workers
    return WorkerSet(
  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py", line 101, in __init__
    remote_spaces = ray.get(self.remote_workers(
ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=26122, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f895041c640>)
  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py", line 541, in __init__
    self.policy_dict = _determine_spaces_for_multi_agent_dict(
  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py", line 1657, in _determine_spaces_for_multi_agent_dict
    raise ValueError(
ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!
2022-04-01 12:36:37,845	INFO trial_runner.py:1176 -- Trial PPO_f81e5_00000: Attempting to restore trial state from last checkpoint.
[36m(PPOTrainer pid=26123)[39m 2022-04-01 12:36:37,832	ERROR worker.py:430 -- Exception raised in creation task: The actor died because of an error raised in its creation task, [36mray::PPOTrainer.__init__()[39m (pid=26123, ip=127.0.0.1, repr=PPOTrainer)
[36m(PPOTrainer pid=26123)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py", line 925, in _init
[36m(PPOTrainer pid=26123)[39m     raise NotImplementedError
[36m(PPOTrainer pid=26123)[39m NotImplementedError
[36m(PPOTrainer pid=26123)
[36m(PPOTrainer pid=26123)[39m During handling of the above exception, another exception occurred:
[36m(PPOTrainer pid=26123)
[36m(PPOTrainer pid=26123)[39m [36mray::PPOTrainer.__init__()[39m (pid=26123, ip=127.0.0.1, repr=PPOTrainer)
[36m(PPOTrainer pid=26123)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py", line 746, in __init__
[36m(PPOTrainer pid=26123)[39m     super().__init__(config, logger_creator, remote_checkpoint_dir,
[36m(PPOTrainer pid=26123)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/tune/trainable.py", line 124, in __init__
[36m(PPOTrainer pid=26123)[39m     self.setup(copy.deepcopy(self.config))
[36m(PPOTrainer pid=26123)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py", line 822, in setup
[36m(PPOTrainer pid=26123)[39m     self.workers = self._make_workers(
[36m(PPOTrainer pid=26123)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py", line 1995, in _make_workers
[36m(PPOTrainer pid=26123)[39m     return WorkerSet(
[36m(PPOTrainer pid=26123)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py", line 101, in __init__
[36m(PPOTrainer pid=26123)[39m     remote_spaces = ray.get(self.remote_workers(
[36m(PPOTrainer pid=26123)[39m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=26122, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f895041c640>)
[36m(PPOTrainer pid=26123)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py", line 541, in __init__
[36m(PPOTrainer pid=26123)[39m     self.policy_dict = _determine_spaces_for_multi_agent_dict(
[36m(PPOTrainer pid=26123)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py", line 1657, in _determine_spaces_for_multi_agent_dict
[36m(PPOTrainer pid=26123)[39m     raise ValueError(
[36m(PPOTrainer pid=26123)[39m ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!
[36m(RolloutWorker pid=26122)[39m 2022-04-01 12:36:37,823	ERROR worker.py:430 -- Exception raised in creation task: The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=26122, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f895041c640>)
[36m(RolloutWorker pid=26122)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py", line 541, in __init__
[36m(RolloutWorker pid=26122)[39m     self.policy_dict = _determine_spaces_for_multi_agent_dict(
[36m(RolloutWorker pid=26122)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py", line 1657, in _determine_spaces_for_multi_agent_dict
[36m(RolloutWorker pid=26122)[39m     raise ValueError(
[36m(RolloutWorker pid=26122)[39m ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!
[36m(RolloutWorker pid=26121)[39m 2022-04-01 12:36:37,823	ERROR worker.py:430 -- Exception raised in creation task: The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=26121, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa236e32640>)
[36m(RolloutWorker pid=26121)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py", line 541, in __init__
[36m(RolloutWorker pid=26121)[39m     self.policy_dict = _determine_spaces_for_multi_agent_dict(
[36m(RolloutWorker pid=26121)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py", line 1657, in _determine_spaces_for_multi_agent_dict
[36m(RolloutWorker pid=26121)[39m     raise ValueError(
[36m(RolloutWorker pid=26121)[39m ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!
[36m(PPOTrainer pid=26119)[39m 2022-04-01 12:36:42,516	INFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.
[36m(PPOTrainer pid=26119)[39m 2022-04-01 12:36:42,516	INFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.
[36m(PPOTrainer pid=26119)[39m 2022-04-01 12:36:42,516	INFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[36m(PPOTrainer pid=26119)[39m 2022-04-01 12:36:47,650	ERROR worker.py:430 -- Exception raised in creation task: The actor died because of an error raised in its creation task, [36mray::PPOTrainer.__init__()[39m (pid=26119, ip=127.0.0.1, repr=PPOTrainer)
[36m(PPOTrainer pid=26119)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py", line 925, in _init
[36m(PPOTrainer pid=26119)[39m     raise NotImplementedError
[36m(PPOTrainer pid=26119)[39m NotImplementedError
[36m(PPOTrainer pid=26119)
[36m(PPOTrainer pid=26119)[39m During handling of the above exception, another exception occurred:
[36m(PPOTrainer pid=26119)
[36m(PPOTrainer pid=26119)[39m [36mray::PPOTrainer.__init__()[39m (pid=26119, ip=127.0.0.1, repr=PPOTrainer)
[36m(PPOTrainer pid=26119)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py", line 746, in __init__
[36m(PPOTrainer pid=26119)[39m     super().__init__(config, logger_creator, remote_checkpoint_dir,
[36m(PPOTrainer pid=26119)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/tune/trainable.py", line 124, in __init__
[36m(PPOTrainer pid=26119)[39m     self.setup(copy.deepcopy(self.config))
[36m(PPOTrainer pid=26119)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py", line 822, in setup
[36m(PPOTrainer pid=26119)[39m     self.workers = self._make_workers(
[36m(PPOTrainer pid=26119)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py", line 1995, in _make_workers
[36m(PPOTrainer pid=26119)[39m     return WorkerSet(
[36m(PPOTrainer pid=26119)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py", line 101, in __init__
[36m(PPOTrainer pid=26119)[39m     remote_spaces = ray.get(self.remote_workers(
[36m(PPOTrainer pid=26119)[39m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=26120, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fbfe5c33610>)
[36m(PPOTrainer pid=26119)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py", line 541, in __init__
[36m(PPOTrainer pid=26119)[39m     self.policy_dict = _determine_spaces_for_multi_agent_dict(
[36m(PPOTrainer pid=26119)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py", line 1657, in _determine_spaces_for_multi_agent_dict
[36m(PPOTrainer pid=26119)[39m     raise ValueError(
[36m(PPOTrainer pid=26119)[39m ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!
[36m(RolloutWorker pid=26120)[39m 2022-04-01 12:36:47,641	ERROR worker.py:430 -- Exception raised in creation task: The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=26120, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fbfe5c33610>)
[36m(RolloutWorker pid=26120)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py", line 541, in __init__
[36m(RolloutWorker pid=26120)[39m     self.policy_dict = _determine_spaces_for_multi_agent_dict(
[36m(RolloutWorker pid=26120)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py", line 1657, in _determine_spaces_for_multi_agent_dict
[36m(RolloutWorker pid=26120)[39m     raise ValueError(
[36m(RolloutWorker pid=26120)[39m ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!
[36m(RolloutWorker pid=26116)[39m 2022-04-01 12:36:47,682	ERROR worker.py:430 -- Exception raised in creation task: The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=26116, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa964438610>)
[36m(RolloutWorker pid=26116)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py", line 541, in __init__
[36m(RolloutWorker pid=26116)[39m     self.policy_dict = _determine_spaces_for_multi_agent_dict(
[36m(RolloutWorker pid=26116)[39m   File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py", line 1657, in _determine_spaces_for_multi_agent_dict
[36m(RolloutWorker pid=26116)[39m     raise ValueError(
