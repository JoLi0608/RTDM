
2022-04-01 12:47:04,643	INFO services.py:1412 -- View the Ray dashboard at [32m[1mhttp://127.0.0.1:8265
== Status ==
Current time: 2022-04-01 12:47:08 (running for 00:00:00.20)
Memory usage on this node: 9.7/16.0 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/5.04 GiB heap, 0.0/2.0 GiB objects
Result logdir: /Users/liwenyu/Downloads/ray_results /PPO
Number of trials: 1/1 (1 PENDING)
+---------------------------+----------+-------+
| Trial name                | status   | loc   |
|---------------------------+----------+-------|
| PPO_Hopper-v2_757ac_00000 | PENDING  |       |
+---------------------------+----------+-------+
[36m(PPOTrainer pid=26594)[39m 2022-04-01 12:47:13,163	INFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.
[36m(PPOTrainer pid=26594)[39m 2022-04-01 12:47:13,163	INFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
== Status ==
Current time: 2022-04-01 12:47:18 (running for 00:00:10.42)
Memory usage on this node: 9.9/16.0 GiB
Using FIFO scheduling algorithm.
Resources requested: 3.0/8 CPUs, 0/0 GPUs, 0.0/5.04 GiB heap, 0.0/2.0 GiB objects
Result logdir: /Users/liwenyu/Downloads/ray_results /PPO
Number of trials: 1/1 (1 RUNNING)
+---------------------------+----------+-----------------+
| Trial name                | status   | loc             |
|---------------------------+----------+-----------------|
| PPO_Hopper-v2_757ac_00000 | RUNNING  | 127.0.0.1:26594 |
+---------------------------+----------+-----------------+
== Status ==
Current time: 2022-04-01 12:47:18 (running for 00:00:10.43)
Memory usage on this node: 9.9/16.0 GiB
Using FIFO scheduling algorithm.
Resources requested: 3.0/8 CPUs, 0/0 GPUs, 0.0/5.04 GiB heap, 0.0/2.0 GiB objects
Result logdir: /Users/liwenyu/Downloads/ray_results /PPO
Number of trials: 1/1 (1 RUNNING)
+---------------------------+----------+-----------------+
| Trial name                | status   | loc             |
|---------------------------+----------+-----------------|
| PPO_Hopper-v2_757ac_00000 | RUNNING  | 127.0.0.1:26594 |
+---------------------------+----------+-----------------+
[36m(PPOTrainer pid=26594)[39m 2022-04-01 12:47:18,428	WARNING util.py:55 -- Install gputil for GPU system monitoring.
[36m(PPOTrainer pid=26594)[39m 2022-04-01 12:47:18,445	INFO trainable.py:495 -- Restored on 127.0.0.1 from checkpoint: /Users/liwenyu/Downloads/ray_results /PPO/PPO_Hopper-v2_757ac_00000_0_2022-04-01_12-47-08/tmpdg7lmlperestore_from_object/checkpoint-4300
[36m(PPOTrainer pid=26594)[39m 2022-04-01 12:47:18,446	INFO trainable.py:503 -- Current state after restoring: {'_iteration': 4300, '_timesteps_total': 17200000, '_time_total': 66449.50474596024, '_episodes_total': 78698}
== Status ==
Current time: 2022-04-01 12:47:23 (running for 00:00:15.44)
Memory usage on this node: 9.9/16.0 GiB
Using FIFO scheduling algorithm.
Resources requested: 3.0/8 CPUs, 0/0 GPUs, 0.0/5.04 GiB heap, 0.0/2.0 GiB objects
Result logdir: /Users/liwenyu/Downloads/ray_results /PPO
Number of trials: 1/1 (1 RUNNING)
+---------------------------+----------+-----------------+
| Trial name                | status   | loc             |
|---------------------------+----------+-----------------|
| PPO_Hopper-v2_757ac_00000 | RUNNING  | 127.0.0.1:26594 |
+---------------------------+----------+-----------------+
Result for PPO_Hopper-v2_757ac_00000:
  agent_timesteps_total: 17204000
  custom_metrics: {}
  date: 2022-04-01_12-47-27
  done: true
  episode_len_mean: 163.65217391304347
  episode_media: {}
  episode_reward_max: 823.5535914804641
  episode_reward_mean: 524.9128113222828
  episode_reward_min: 25.164825188839316
  episodes_this_iter: 23
  episodes_total: 78721
  experiment_id: 1f82ae63d4e242daa3e6ce4fe971dddc
  hostname: dhcp-10-249-173-232.eduroam.wireless.private.cam.ac.uk
  info:
    learner:
      default_policy:
        custom_metrics: {}
        learner_stats:
          allreduce_latency: 0.0
          cur_kl_coeff: 0.20000000000000004
          cur_lr: 5.0000000000000016e-05
          entropy: 10.936694981462212
          entropy_coeff: 0.0
          kl: 0.04223712021047667
          policy_loss: -0.08631036622219429
          total_loss: 2106.753493151101
          vf_explained_var: 0.6404260923144638
          vf_loss: 2106.831375831686
        model: {}
    num_agent_steps_sampled: 17204000
    num_agent_steps_trained: 17204000
    num_steps_sampled: 17204000
    num_steps_trained: 17204000
    num_steps_trained_this_iter: 4000
  iterations_since_restore: 1
  node_ip: 127.0.0.1
  num_healthy_workers: 2
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.907142857142862
    ram_util_percent: 61.892857142857125
  pid: 26594
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12618255250532645
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.24240246378162983
    mean_inference_ms: 0.9386247179797947
    mean_raw_obs_processing_ms: 0.11324763875651767
  time_since_restore: 9.538656949996948
  time_this_iter_s: 9.538656949996948
  time_total_s: 66459.04340291023
  timers:
    learn_throughput: 622.279
    learn_time_ms: 6427.98
    load_throughput: 8564173.558
    load_time_ms: 0.467
    sample_throughput: 1280.594
    sample_time_ms: 3123.551
    update_time_ms: 3.837
  timestamp: 1648813647
  timesteps_since_restore: 4000
  timesteps_this_iter: 4000
  timesteps_total: 17204000
  training_iteration: 4301
  trial_id: 757ac_00000
== Status ==
Current time: 2022-04-01 12:47:28 (running for 00:00:20.03)
Memory usage on this node: 9.9/16.0 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/5.04 GiB heap, 0.0/2.0 GiB objects
Result logdir: /Users/liwenyu/Downloads/ray_results /PPO
Number of trials: 1/1 (1 TERMINATED)
+---------------------------+------------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name                | status     | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|---------------------------+------------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_Hopper-v2_757ac_00000 | TERMINATED | 127.0.0.1:26594 |   4301 |            66459 | 17204000 |  524.913 |              823.554 |              25.1648 |            163.652 |
+---------------------------+------------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
difficulty level: 0
[36m(PPOTrainer pid=26594)[39m 2022-04-01 12:47:28,018	WARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!
2022-04-01 12:47:28,375	INFO tune.py:639 -- Total run time: 20.38 seconds (20.01 seconds for the tuning loop).
Traceback (most recent call last):
  File "/Users/liwenyu/Documents/GitHub/RTDM/loadmodel.py", line 123, in <module>
    reward_ave = play(env, trainer, 800, asy = 0)
  File "/Users/liwenyu/Documents/GitHub/RTDM/loadmodel.py", line 89, in play
    action = trainer.compute_single_action(obs)
AttributeError: 'ExperimentAnalysis' object has no attribute 'compute_single_action'
Traceback (most recent call last):
  File "/Users/liwenyu/Documents/GitHub/RTDM/loadmodel.py", line 123, in <module>
    reward_ave = play(env, trainer, 800, asy = 0)
  File "/Users/liwenyu/Documents/GitHub/RTDM/loadmodel.py", line 89, in play
    action = trainer.compute_single_action(obs)
AttributeError: 'ExperimentAnalysis' object has no attribute 'compute_single_action'